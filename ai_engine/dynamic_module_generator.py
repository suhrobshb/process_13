"""
Dynamic Module Generator
=========================

This module is the core of the AI Engine's adaptive learning capability. It is
responsible for taking the structured workflow, analyzed by the AILearningEngine,
and converting it into an executable, sandboxed, and validated Python module.

The process follows these key steps:
1.  **Ingestion**: Receives a structured workflow dictionary containing nodes, edges,
    AI-generated descriptions, and confidence scores.
2.  **Code Generation**: Using Jinja2 templates, it dynamically writes a Python
    module. This module's `run()` function will instantiate and execute the
    appropriate high-level runners (`DesktopRunner`, `BrowserRunner`, etc.)
    for each step, passing the raw action data.
3.  **Test Generation**: Simultaneously, it generates a corresponding `pytest` file
    with mocked tests to validate that the generated module calls the correct
    runners with the correct parameters.
4.  **Validation**: It runs the generated tests in a sandboxed environment to ensure
    the generated code is logically sound.
5.  **Approval**: If all tests pass, the generated module is marked as "approved"
    and is ready to be executed by the main workflow engine.

This architecture allows the AI Engine to learn from user behavior and adapt to
any application or workflow, fulfilling the vision of a truly intelligent
automation platform.
"""

import os
import json
import logging
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import time

import pytest
from jinja2 import Template

# Configure logging
logger = logging.getLogger(__name__)

# --- Jinja2 Templates for Intelligent Code Generation ---

# Template for the executable workflow module.
# This template now generates code that calls our high-level runners.
MODULE_TEMPLATE = Template("""
\"\"\"
Dynamically Generated Workflow Module
-------------------------------------
Workflow ID: {{ workflow.name }} ({{ workflow.id }})
Generated on: {{ generation_date }}
Overall Confidence: {{ workflow.overall_confidence }}

This module was automatically generated by the AI Engine. It orchestrates the
execution of a business process by calling the appropriate high-level runners.
\"\"\"

import logging
from ai_engine.enhanced_runners.desktop_runner import DesktopRunner
from ai_engine.enhanced_runners.browser_runner import BrowserRunner
from ai_engine.workflow_runners import RunnerFactory

logger = logging.getLogger(__name__)

# --- Step Definitions ---
WORKFLOW_NODES = {{ workflow_nodes_json }}

def run(context: dict) -> dict:
    \"\"\"
    Main execution function for this dynamically generated workflow.
    \"\"\"
    logger.info(f"Starting execution of workflow: {{ workflow.name }}")
    execution_results = {}
    
    # In a real engine, this would use a topological sort of the edges.
    # For this demonstration, we execute in the order provided.
    for node in WORKFLOW_NODES:
        step_id = node.get("id")
        step_type = node.get("type")
        step_data = node.get("data", {})
        
        logger.info(f"Executing step '{step_id}' (Type: {step_type})")
        
        # Check confidence score before execution
        confidence = step_data.get("confidence_score", 1.0)
        if confidence < 0.75:
            logger.warning(
                f"Confidence score for step '{step_id}' is low ({confidence}). "
                f"Proceeding, but this step may require user review."
            )

        try:
            # Instantiate the correct runner for the step type
            if step_type == "desktop":
                runner = DesktopRunner(step_id, {"actions": step_data.get("raw_actions", [])})
                result = runner.execute()
            elif step_type == "browser":
                runner = BrowserRunner(step_id, {"actions": step_data.get("raw_actions", [])})
                result = runner.execute()
            else:
                # Use the standard factory for other types like shell, http, llm
                runner = RunnerFactory.create_runner(step_type, step_id, step_data)
                result = runner.execute(context)

            if not result.get("success"):
                raise Exception(result.get("error", "An unknown error occurred in runner."))

            execution_results[step_id] = {"status": "success", "output": result.get("result", result.get("results"))}
            # Update context for subsequent steps
            context[step_id] = result.get("result", {})

        except Exception as e:
            logger.error(f"Step '{step_id}' failed: {e}", exc_info=True)
            execution_results[step_id] = {"status": "failure", "error": str(e)}
            # Stop execution on failure
            break
            
    logger.info(f"Workflow '{{ workflow.name }}' finished execution.")
    return execution_results

if __name__ == "__main__":
    # Example of how to run this module directly for testing
    run({})
""")

# Template for the validation test file.
# This template now mocks the high-level runners.
TEST_TEMPLATE = Template("""
\"\"\"
Dynamically Generated Validation Test
-------------------------------------
Workflow ID: {{ workflow.name }} ({{ workflow.id }})
Generated on: {{ generation_date }}

This test file validates the logic of the generated module by ensuring it calls
the correct runners with the correct parameters.
\"\"\"

import pytest
from unittest.mock import patch, MagicMock

# Import the module to be tested
from . import {{ module_name }}

# Mock the runners to prevent real automation during tests
@patch('{{ module_name }}.DesktopRunner')
@patch('{{ module_name }}.BrowserRunner')
@patch('{{ module_name }}.RunnerFactory')
def test_workflow_orchestration(MockRunnerFactory, MockBrowserRunner, MockDesktopRunner, mocker):
    \"\"\"
    Tests that the main run() function correctly orchestrates the workflow
    by calling the appropriate runners for each step.
    \"\"\"
    # Arrange
    # Set up mock return values for the runner instances
    MockDesktopRunner.return_value.execute.return_value = {"success": True, "result": "desktop_ok"}
    MockBrowserRunner.return_value.execute.return_value = {"success": True, "result": "browser_ok"}
    
    # Mock for other runner types like 'shell' or 'http'
    mock_other_runner = MagicMock()
    mock_other_runner.execute.return_value = {"success": True, "result": "other_ok"}
    MockRunnerFactory.create_runner.return_value = mock_other_runner

    # Act
    results = {{ module_name }}.run({})

    # Assert
    # Verify that the correct runners were instantiated and executed for each step
    {% for node in workflow.nodes %}
    {% if node.type == 'desktop' %}
    MockDesktopRunner.assert_any_call("{{ node.id }}", {"actions": {{ node.data.raw_actions | tojson }} })
    {% elif node.type == 'browser' %}
    MockBrowserRunner.assert_any_call("{{ node.id }}", {"actions": {{ node.data.raw_actions | tojson }} })
    {% else %}
    MockRunnerFactory.create_runner.assert_any_call("{{ node.type }}", "{{ node.id }}", {{ node.data | tojson }})
    {% endif %}
    {% endfor %}
    
    # Check that the final results dictionary is correctly populated
    assert len(results) == {{ workflow.nodes|length }}
    assert results["{{ workflow.nodes[0].id }}"]["status"] == "success"
""")


class DynamicModuleGenerator:
    """
    Generates and validates executable Python modules from AI-analyzed workflows.
    """

    def __init__(self, structured_workflow: Dict[str, Any]):
        """
        Initializes the generator with a structured workflow from the AILearningEngine.

        Args:
            structured_workflow (Dict[str, Any]): A dictionary representing the
                workflow, containing nodes, edges, and metadata.
        """
        self.workflow = structured_workflow
        self.workflow_id = structured_workflow.get("id", f"wf_{int(time.time())}")
        self.module_dir = Path(f"storage/dynamic_modules/{self.workflow_id}")
        self.module_name = f"workflow_module_{self.workflow_id}"
        self.module_path = self.module_dir / f"{self.module_name}.py"
        self.test_path = self.module_dir / f"test_{self.module_name}.py"

        # Ensure the target directory exists
        self.module_dir.mkdir(parents=True, exist_ok=True)
        (self.module_dir / "__init__.py").touch(exist_ok=True)

    def _generate_code(self) -> Tuple[str, str]:
        """
        Generates the Python module and its test file from the structured workflow.
        """
        generation_date = datetime.utcnow().isoformat()
        
        # Generate module code
        module_code = MODULE_TEMPLATE.render(
            workflow=self.workflow,
            workflow_nodes_json=json.dumps(self.workflow.get("nodes", []), indent=4),
            generation_date=generation_date
        )
        
        # Generate test code
        test_code = TEST_TEMPLATE.render(
            workflow=self.workflow,
            module_name=self.module_name,
            generation_date=generation_date
        )
        
        return module_code, test_code

    def _save_files(self, module_code: str, test_code: str):
        """Saves the generated code to files."""
        logger.info(f"Saving generated module to {self.module_path}")
        with open(self.module_path, "w", encoding="utf-8") as f:
            f.write(module_code)
            
        logger.info(f"Saving validation test to {self.test_path}")
        with open(self.test_path, "w", encoding="utf-8") as f:
            f.write(test_code)

    def _run_validation_tests(self) -> bool:
        """
        Runs the generated validation tests using pytest.
        """
        logger.info(f"Running validation tests for {self.module_name}...")
        
        # Add the parent directory to sys.path to allow relative imports in the test
        sys.path.insert(0, str(self.module_dir.parent))
        
        try:
            result_code = pytest.main([str(self.test_path), "-q", "--no-header", "--no-summary"])
        finally:
            # Clean up sys.path
            sys.path.pop(0)
        
        if result_code == pytest.ExitCode.OK:
            logger.info("✅ Validation tests passed successfully.")
            return True
        else:
            logger.error(f"❌ Validation tests failed with exit code {result_code}.")
            return False

    def generate_and_validate(self) -> Optional[Path]:
        """
        Orchestrates the full process of generation and validation.

        Returns:
            The path to the validated module if successful, otherwise None.
        """
        logger.info(f"Starting dynamic module generation for workflow: {self.workflow_id}")
        
        # 1. Generate code from the structured workflow
        module_code, test_code = self._generate_code()
        
        # 2. Save files
        self._save_files(module_code, test_code)
        
        # 3. Run validation tests
        is_valid = self._run_validation_tests()
        
        # 4. Return module path if valid
        if is_valid:
            logger.info(f"Module for workflow {self.workflow_id} is validated and ready.")
            return self.module_path
        else:
            logger.error(f"Generated module for workflow {self.workflow_id} failed validation.")
            return None

# --- Example Usage ---
if __name__ == "__main__":
    # This is a demonstration of how the generator would be used.
    
    # 1. Sample structured workflow data (output from AILearningEngine)
    sample_structured_workflow = {
        "id": "demo_po_processing",
        "name": "Process New Purchase Order",
        "overall_confidence": 0.92,
        "nodes": [
            {
                "id": "step1_read_email",
                "type": "desktop",
                "data": {
                    "label": "Read PO from Email",
                    "description": "AI will open Outlook, find the email, and open the attached PDF.",
                    "confidence_score": 0.95,
                    "raw_actions": [
                        {"type": "click", "x": 150, "y": 200},
                        {"type": "double_click", "x": 400, "y": 500}
                    ]
                }
            },
            {
                "id": "step2_enter_in_crm",
                "type": "browser",
                "data": {
                    "label": "Enter PO in Web CRM",
                    "description": "AI will open Chrome, navigate to the CRM, and enter the PO details.",
                    "confidence_score": 0.88,
                    "raw_actions": [
                        {"type": "goto", "url": "https://crm.example.com"},
                        {"type": "fill", "selector": "#po-number", "text": "PO12345"}
                    ]
                }
            }
        ],
        "edges": [{"source": "step1_read_email", "target": "step2_enter_in_crm"}]
    }
    
    # 2. Instantiate and run the generator
    generator = DynamicModuleGenerator(structured_workflow=sample_structured_workflow)
    validated_module_path = generator.generate_and_validate()
    
    # 3. Check the result
    if validated_module_path:
        print(f"\n✅ Successfully generated and validated module: {validated_module_path}")
        print("\n--- Generated Module Content ---")
        with open(validated_module_path, 'r') as f:
            print(f.read())
        print("\n--- Generated Test Content ---")
        test_file = validated_module_path.parent / f"test_{validated_module_path.stem}.py"
        with open(test_file, 'r') as f:
            print(f.read())
    else:
        print("\n❌ Failed to generate a valid module.")
